import streamlit as st
from streamlit_webrtc import webrtc_streamer, AudioProcessorBase
from deep_translator import GoogleTranslator
import edge_tts
import asyncio
import tempfile
import av

# ğŸŒ Taalinstellingen
bron_taal = st.selectbox("Welke taal spreek je?", ["fr", "pt", "nl", "zh-CN", "ln", "en", "es", "de"])
doel_taal = st.selectbox("Welke taal wil je horen?", ["nl", "fr", "pt", "zh-CN", "ln", "en", "es", "de"])

# ğŸ§ Tekst uitspreken
async def spreek_tekst(tekst, taalcode):
    stemmap = {
        "nl": "nl-NL-MaartenNeural",
        "fr": "fr-FR-DeniseNeural",
        "pt": "pt-BR-AntonioNeural",
        "zh-CN": "zh-CN-XiaoxiaoNeural",
        "es": "es-ES-AlvaroNeural",
        "de": "de-DE-ConradNeural",
        "en": "en-US-AriaNeural",
        "ln": "pt-BR-AntonioNeural",
    }
    stem = stemmap.get(taalcode, "en-US-AriaNeural")
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmpfile:
        await edge_tts.Communicate(tekst, stem).save(tmpfile.name)
        return tmpfile.name

# ğŸ§  Dummy audioverwerker (geen herkenning, alleen simulatie)
class AudioProcessor(AudioProcessorBase):
    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        # Simuleer een zin (later vervangen door echte herkenning)
        zin = "Bonjour tout le monde"
        vertaling = GoogleTranslator(source=bron_taal, target=doel_taal).translate(zin)
        st.write(f"ğŸ—£ï¸ Origineel: {zin}")
        st.write(f"ğŸŒ Vertaling: {vertaling}")
        audiobestand = asyncio.run(spreek_tekst(vertaling, doel_taal))
        with open(audiobestand, "rb") as af:
            st.audio(af.read(), format="audio/mp3")
        return frame

# ğŸš€ Streamlit interface
st.title("ğŸŒ Live Spraakvertaler voor Telefoon en PC")
st.markdown("ğŸ™ï¸ Spreek live via je microfoon. Vertaling verschijnt automatisch en wordt uitgesproken.")

webrtc_streamer(
    key="live-vertaler",
    audio_processor_factory=AudioProcessor,
    media_stream_constraints={"audio": True, "video": False},
    async_processing=True,
)
